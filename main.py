"""
There are 20 emotions and for each emotion there is an AI picture representing that emotion and a normal picture representing that emotion.
Ask AI to describe with one word the emotion that they get from each picture. Ask Open AI and Gemini. For each AI ask twice for control.
Load responses from humans to the same question.

For every emotion make a two charts, one for AI resposnes and one for Human responses. Represent each response emotion as a point in a 2D space, vizualise to see if the AI responses are placed similarly to the human responses. ALso place the ground truth emotion in the same space.
For each emotion calculate the distance between the AI responses and the human responses. Calculate the average distance for all emotions.

See if AI has more accurate responses for images generated by AI. 
"""
import os
from dotenv import load_dotenv
import google.generativeai as genai
import google.ai.generativelanguage as glm
from openai import OpenAI
from PIL import Image
import base64
import pathlib
import requests
import pandas as pd
import json
import matplotlib.pyplot as plt
from sklearn.manifold import TSNE
from gensim.models import KeyedVectors
import numpy as np
from scipy.spatial.distance import euclidean

load_dotenv() 

GOOGLE_API_KEY=os.getenv("GOOGLE_API_KEY") 
OPENAI_API_KEY=os.getenv("OPENAI_API_KEY")

genai.configure(api_key=GOOGLE_API_KEY)
model = genai.GenerativeModel('gemini-pro')
model_vision = genai.GenerativeModel('gemini-pro-vision')
client = OpenAI(api_key = OPENAI_API_KEY)

glove_path = 'vector-models/glove.6B.50d.txt'  # Specify the correct path to the GloVe file
glove_model = KeyedVectors.load_word2vec_format(glove_path, binary=False, no_header=True)

iter = 4 # amount of responses per AI on a single image

def load_human_responses(image_names):
    file_path = 'Emotion Recognition Survey.csv'
    data = pd.read_csv(file_path)
    # Dictionary to store responses for each image
    response_dict = {}
    
    # Extract only response columns (ignoring score and feedback columns)
    response_columns = [col for col in data.columns if "What ONE WORD" in col and "Score" not in col and "Feedback" not in col]

    for i, image_name in enumerate(image_names):
        # Fetch responses for each image from the data
        responses = data[response_columns[i]].dropna().tolist()
        response_dict[image_name] = {"human_responses": responses, "ai_responses": []}

    return response_dict
    

# create list with image names from the folder
def get_image_names(folder_path):
    image_names = []
    for filename in os.listdir(folder_path):
        if filename.endswith(".jpg"):
            image_names.append(filename)
    image_names.sort()
    temp = ""
    for i in range(0,len(image_names),2):
        temp = image_names[i]
        image_names[i] = image_names[i+1]
        image_names[i+1] = temp
    return image_names

def get_image_emotion_gemini(image_path):
    prompt = "What ONE WORD describes the emotion in this photo best?"
    response = model_vision.generate_content(
        glm.Content(
            parts = [
                glm.Part(text=prompt),
                glm.Part(
                    inline_data=glm.Blob(
                        # mime_type='image/'+image_path.split(".")[-1],
                        mime_type='image/jpeg',
                        data=pathlib.Path(image_path).read_bytes()
                    )
                ),
            ],
    ))
    print(response)
    return response.text

def encode_image(image_path):
  with open(image_path, "rb") as image_file:
    return base64.b64encode(image_file.read()).decode('utf-8')

def get_image_emotion_openai(image_path):
    base64_image = encode_image(image_path)

    headers = {
    "Content-Type": "application/json",
    "Authorization": f"Bearer {OPENAI_API_KEY}"
    }

    payload = {
    "model": "gpt-4-turbo",
    "messages": [
        {
        "role": "user",
        "content": [
            {
            "type": "text",
            "text": "What ONE WORD describes the emotion in this photo best?"
            },
            {
            "type": "image_url",
            "image_url": {
                "url": f"data:image/jpeg;base64,{base64_image}"
            }
            }
        ]
        }
    ],
    "max_tokens": 300
    }

    response = requests.post("https://api.openai.com/v1/chat/completions", headers=headers, json=payload)

    # print(response.json())
    emotion = response.json()["choices"][0]["message"]["content"]
    return emotion




def main():
    folder_path = "Images"
    image_names = get_image_names(folder_path)
    results = load_human_responses(image_names)
    # print(results)
    # exit()
    print(image_names)
    try:
        for image_name in image_names:
            image_path = folder_path + "/" + image_name
            print(f"Processing image: {image_name}")
            for i in range(iter):
                # print(get_image_emotion_gemini(image_path))
                emotion = get_image_emotion_openai(image_path)
                results[image_name]["ai_responses"].append(emotion)
    except:
        print("Error")
        print(results)
    print(results)


def get_vector(word):
    """Return the vector for a word from the GloVe model, handle missing words safely."""
    try:
        return glove_model[word]
    except KeyError:
        return np.zeros(glove_model.vector_size)
    
def check_vectors():
    word = "shock"
    print(get_vector(word))
    vector1 = get_vector("shock")
    vectors = []
    for x in range(20):
        vectors.append(get_vector(word))
    for vector in vectors:
        print(euclidean(vector1, vector))
        print(vector1 == vector)

def plot_responses2(data):
    for image_name, responses in data.items():
        unique_words = set()
        human_words = [word.lower().split(".")[0] for word in responses['human_responses']]
        ai_words = [word.lower().split(".")[0] for word in responses['ai_responses']]
        unique_words.update([word.lower() for word in responses['human_responses']])
        unique_words.update([word.lower() for word in responses['ai_responses']])
        ground_truth = image_name.split(".")[0].split("_")[0].lower()
        unique_words.add(ground_truth)

        word_vectors = np.array([get_vector(word) for word in unique_words])
        # Adjust perplexity to be less than the number of samples but not less than 5
        perplexity_value = min(len(word_vectors) - 1, 30)  # Default perplexity is often 30
        perplexity_value = max(perplexity_value, 5)  # Ensure it's at least 5 for meaningful results

        tsne = TSNE(n_components=2, perplexity=perplexity_value, random_state=42)
        vectors_2d = tsne.fit_transform(word_vectors)
        word_to_2dvector = {}
        for i, word in enumerate(unique_words):
            word_to_2dvector[word] = vectors_2d[i]

        plt.figure(figsize=(10, 10))
        repeated_words = {}
        used_coords = {}
        for word in human_words:
            s = 100
            if word in repeated_words:
                s = 150 * repeated_words[word] + 1
            plt.scatter(word_to_2dvector[word][0], word_to_2dvector[word][1], color='blue', s=s)
            if word not in repeated_words:
                text_offset = 0
                text = f"human: {word}"
                if str(word_to_2dvector[word]) in used_coords:
                    text_offset = 100*used_coords[str(word_to_2dvector[word])]
                    text = f", {word}"
                plt.annotate(text, 
                xy=(word_to_2dvector[word][0], word_to_2dvector[word][1]), 
                xytext=(text_offset, -20),  # Shifts text by 10 points down
                textcoords='offset points',  # Uses offset in points
                ha='center') 
                used_coords[str(word_to_2dvector[word])] = used_coords.get(str(word_to_2dvector[word]), 0) + 1
            repeated_words[word] = repeated_words.get(word, 0) + 1
        repeated_words = {}
        used_coords = {}
        for word in ai_words:
            plt.scatter(word_to_2dvector[word][0], word_to_2dvector[word][1], color='red')
            if word not in repeated_words:
                text_offset = 0
                text = f"ai: {word}"
                if str(word_to_2dvector[word]) in used_coords:
                    text_offset = 100*used_coords[str(word_to_2dvector[word])]
                    text = f", {word}"
                plt.annotate(text, 
                xy=(word_to_2dvector[word][0], word_to_2dvector[word][1]), 
                xytext=(text_offset, -10),  # Shifts text by 10 points down
                textcoords='offset points',  # Uses offset in points
                ha='center') 
                used_coords[str(word_to_2dvector[word])] = used_coords.get(str(word_to_2dvector[word]), 0) + 1
            repeated_words[word] = repeated_words.get(word, 0) + 1
        plt.scatter(word_to_2dvector[ground_truth][0], word_to_2dvector[ground_truth][1], color='green', s=100) 
        plt.annotate(f"ground truth: {ground_truth}", xy=(word_to_2dvector[ground_truth][0], word_to_2dvector[ground_truth][1]),xytext=(0, 10),  # Shifts text by 10 points down
                textcoords='offset points',  # Uses offset in points
                ha='center')
        plt.title(image_name)
        plt.legend()
        plt.grid(True)
        # plt.show()
        plt.savefig(f"plots2/{image_name}-vectors.png")
        # exit()
def plot_responses(data):
    # Collect all unique words
    unique_words = set()
    for image_name, responses in data.items():
        unique_words.update([word.lower() for word in responses['human_responses']])
        unique_words.update([word.lower() for word in responses['ai_responses']])
        unique_words.add(image_name.split(".")[0].split("_")[0].lower())
    print(unique_words)
    exit()

    # Map words to vectors
    word_vectors = np.array([get_vector(word) for word in unique_words])

    # Apply t-SNE to all unique word vectors
    tsne = TSNE(n_components=2, random_state=42)
    vectors_2d = tsne.fit_transform(word_vectors)
    word_to_coord = dict(zip(unique_words, vectors_2d))

    # Plot each set of responses
    for image_name, responses in data.items():
        plt.figure(figsize=(8, 6))

        # Get coordinates for the current image's words
        words = [word.lower() for word in responses['human_responses'] + responses['ai_responses']]
        ground_truth = image_name.split(".")[0].split("_")[0].lower()
        words.append(ground_truth)
        
        coords = np.array([word_to_coord[word] for word in words])
        human_count = len(responses['human_responses'])
        ai_count = len(responses['ai_responses'])

        # Plotting
        plt.scatter(coords[:human_count, 0], coords[:human_count, 1], color='blue', label='Human Responses')
        plt.scatter(coords[human_count:human_count + ai_count, 0], coords[human_count:human_count + ai_count, 1], color='red', label='AI Responses')
        plt.scatter(coords[-1, 0], coords[-1, 1], color='green', s=100, label='Ground Truth')  # Emphasize ground truth

        # Annotate words
        for i, word in enumerate(words):
            plt.annotate(word, (coords[i, 0], coords[i, 1]))

        plt.title(image_name)
        plt.legend()
        plt.grid(True)
        plt.savefig(f"plots/{image_name}-vectors.png")
        # plt.show()


def calculate_distances_and_plot(data):
    # Collect all unique words
    unique_words = set()
    for image_name, responses in data.items():
        unique_words.update([word.lower() for word in responses['human_responses']])
        unique_words.update([word.lower() for word in responses['ai_responses']])
        unique_words.add(image_name.split(".")[0].split("_")[0].lower())

    # Map words to vectors
    word_vectors = np.array([get_vector(word) for word in unique_words])

    # Apply t-SNE to all unique word vectors
    tsne = TSNE(n_components=2, random_state=42)
    vectors_2d = tsne.fit_transform(word_vectors)
    word_to_coord = dict(zip(unique_words, vectors_2d))

    # Calculate distances and store results
    results = {}
    for image_name, responses in data.items():
        ground_truth = image_name.split(".")[0].split("_")[0].lower()
        gt_coord = word_to_coord[ground_truth]

        human_coords = [word_to_coord[word.lower()] for word in responses['human_responses']]
        ai_coords = [word_to_coord[word.lower()] for word in responses['ai_responses'] if word.lower() in word_to_coord]

        human_distances = [euclidean(coord, gt_coord) for coord in human_coords]
        ai_distances = [euclidean(coord, gt_coord) for coord in ai_coords]

        results[image_name] = {
            'human_average_distance': np.mean(human_distances) if human_distances else 0,
            'ai_average_distance': np.mean(ai_distances) if ai_distances else 0
        }

    # Plotting the results for human responses
    labels = list(results.keys())
    human_distances = [res['human_average_distance'] for res in results.values()]
    ai_distances = [res['ai_average_distance'] for res in results.values()]

    # Determine the maximum y-value for consistent axis limits
    max_y = max(max(human_distances), max(ai_distances)) * 1.1  # Add 10% headroom

    x = np.arange(len(labels))  # the label locations

    # Human responses plot
    plt.figure(figsize=(10, 6))
    plt.bar(x, human_distances, color='blue', label='Human Responses')
    plt.ylabel('Average Distance to Ground Truth')
    plt.title('Human Responses: Average Distances to Ground Truth')
    plt.xticks(x, labels, rotation=45, ha="right")
    plt.ylim(0, max_y)
    plt.legend()
    plt.tight_layout()
    plt.savefig("human_responses.png")
    plt.show()

    # AI responses plot
    plt.figure(figsize=(10, 6))
    plt.bar(x, ai_distances, color='red', label='AI Responses')
    plt.ylabel('Average Distance to Ground Truth')
    plt.title('AI Responses: Average Distances to Ground Truth')
    plt.xticks(x, labels, rotation=45, ha="right")
    plt.ylim(0, max_y)
    plt.legend()
    plt.tight_layout()
    plt.savefig("ai_responses.png")
    plt.show()

def load_data():
    f = open("data.json")
    data = json.load(f)
    return data

if __name__ == "__main__":
    # main()
    data = load_data()
    plot_responses2(data)
    # calculate_distances_and_plot(data)
    # check_vectors()

"""
{'anger_ai.jpg': {'human_responses': ['Authoritative ', 'Sarcastic ', 'Mad', 'disappointed', 'concerned ', 'Angry', 'Angry', 'hard', 'annoyed'], 'ai_responses': []}, 'anger.jpg': {'human_responses': ['Angry', 'Angsty', 'Angry', 'annoyed', 'angry ', 'Frustrated', 'Mad', 'angry', 'annoyed', 'cross'], 'ai_responses': []}, 'boredom_ai.jpg': {'human_responses': ['Grave ', 'Intense ', 'Blank', 'judging', 'blank', 'Concentrated', 'Bored', 'stoic', 'disgusted'], 'ai_responses': []}, 'boredom.jpg': {'human_responses': ['Irritated ', 'Confident ', 'Pensive', 'skeptical', 'boredom', 'Tired', 'Numb', 'in thought', 'romantic', 'neutral'], 'ai_responses': []}, 'calmness_ai.jpg': {'human_responses': ['Stoic', 'Intense ', 'Pensive', 'neutral', 'blank', 'Serious', 'Blank', 'blank', 'nothing', 'focused'], 'ai_responses': []}, 'calmness.jpg': {'human_responses': ['Relaxed', 'Peaceful', 'Peaceful', 'relaxed', 'relaxed ', 'Serene', 'Relaxed', 'relaxed', 'calm', 'meditative'], 'ai_responses': []}, 'confusion_ai.jpg': {'human_responses': ['Sincere ', 'Intense ', 'Concerned', 'blank', 'Serious', 'Intimidating', 'blank', 'calm', 'neutral'], 'ai_responses': []}, 'confusion.jpg': {'human_responses': ['Surprised', 'Disheartened ', 'Hurt', 'hurt', 'confused ', 'Confused', 'Sad', 'thinking', 'guilty', 'questioning'], 'ai_responses': []}, 'curiosity_ai.jpg': {'human_responses': ['Solemn ', 'Content ', 'Interested', 'satisfied', 'content ', 'Inspired', 'satisfied', 'interested'], 'ai_responses': []}, 'curiosity.jpg': {'human_responses': ['Pondering ', 'Curious', 'Confused', 'confused ', 'Confused', 'Confused', 'thoughtless', 'nothing'], 'ai_responses': []}, 'disappointment_ai.jpg': {'human_responses': ['Serious ', 'Sly ', 'Intense', 'thinking', 'concentrated ', 'Concentrated', 'Serious', 'handsome', 'disgusted'], 'ai_responses': []}, 'disappointment.jpg': {'human_responses': ['Grim ', 'Pensive ', 'Disapproving', 'concern', 'disgust ', 'Disappointed', 'Upset', 'sad', 'hostile', 'concerned'], 'ai_responses': []}, 'disgust_ai.jpg': {'human_responses': ['Confused', 'Surprised ', 'Confused', 'disgusted', 'disgust ', 'surprised', 'Confused', 'confused', 'Angry', 'worried'], 'ai_responses': []}, 'disgust.jpg': {'human_responses': ['Disgust ', 'Disgusted ', 'Disgusted', 'disgusted', 'disgust ', 'Angry', 'Confused', 'disgusted', 'disgusted'], 'ai_responses': []}, 'excitement_ai.jpg': {'human_responses': ['Excitement ', 'Happy', 'Excited', 'elated', 'ecstatic ', 'Excited', 'Excited', 'happy', 'joyful', 'delighted'], 'ai_responses': []}, 'excitement.jpg': {'human_responses': ['Free', 'Happy', 'Giggly', 'happy', 'joy', 'Happy', 'Joyful', 'glee', 'satisfied', 'happy'], 'ai_responses': []}, 'fear_ai.jpg': {'human_responses': ['Shock ', 'Shocked ', 'Shocked', 'scared', 'shocked', 'Shocked', 'Shocked', 'suprised', 'scared'], 'ai_responses': []}, 'fear.jpg': {'human_responses': ['Apprehension ', 'Shy', 'Frustrated', 'disgusted', 'anger ', 'Scared', 'Disgusted', 'dissatisfied', 'disgusted', 'rejection'], 'ai_responses': []}, 'gratitude_ai.jpg': {'human_responses': ['Profound ', 'Joyous ', 'Touched', 'empathetic', 'gratitude ', 'Enjoyable', 'compassio', 'thankful', 'grateful'], 'ai_responses': []}, 'gratitude.jpg': {'human_responses': ['Free', 'Happy', 'Free', 'joyful', 'carefree ', 'Joyful', 'Carefree', 'content', 'peaceful', 'ecstatic'], 'ai_responses': []}, 'happiness_ai.jpg': {'human_responses': ['Happy', 'Content ', 'Happy', 'happy', 'happiness ', 'Content', 'Happy', 'happy', 'happy', 'happy'], 'ai_responses': []}, 'happiness.jpg': {'human_responses': ['Happy', 'Happy', 'Happy', 'happy', 'happiness ', 'Happy', 'Smiley', 'happy', 'delighted'], 'ai_responses': []}, 'hate_ai.jpg': {'human_responses': ['Fear', 'Disgusted ', 'Angry', 'angry?', 'anger ', 'Infuriated', 'Frightened', 'angry', 'obnoxious', 'angry'], 'ai_responses': []}, 'hate.jpg': {'human_responses': ['Frustrated', 'Angry ', 'Ecstatic', 'frustrated ', 'energized', 'Frustrated', 'excited', 'excited', 'powerful'], 'ai_responses': []}, 'jealousy_ai.jpg': {'human_responses': ['Annoyed', 'Confused ', 'Disappointed', 'disgusted', 'upset ', 'Sad', 'Upset', 'sad', 'confused', 'worried'], 'ai_responses': []}, 'jealousy.jpg': {'human_responses': ['Insecure', 'Angry ', 'Suspicious', 'skeptical', 'resentment ', 'Self concious', 'Stern', 'lost', 'nothing'], 'ai_responses': []}, 'loneliness_ai.jpg': {'human_responses': ['Inquisitive ', 'Observant', 'annoyed', 'focused ', 'Focused', 'Blank', 'blank', 'disgusted', 'pensive'], 'ai_responses': []}, 'loneliness.jpg': {'human_responses': ['Depressed', 'Depressed', 'Depressed', 'sad', 'sadness ', 'Sad', 'Depressed', 'sad', 'gloomy', 'sad'], 'ai_responses': []}, 'pride_ai.jpg': {'human_responses': ['Proud ', 'Proud ', 'Proud', 'confident', 'pride ', 'Proud', 'Proud', 'proud', 'proud', 'puffy'], 'ai_responses': []}, 'pride.jpg': {'human_responses': ['Pretentious ', 'Proud', 'Confident', 'confident', 'determination ', 'Determined', 'Narcissistic', 'proud', 'proud', 'proud'], 'ai_responses': []}, 'sadness.jpg': {'human_responses': ['Welcoming ', 'Confident ', 'Interested', 'interested', 'confused  ', 'Playful', 'Blank', 'handsome', 'confused', 'happy'], 'ai_responses': []}, 'relief_ai.jpg': {'human_responses': ['Overjoyed ', 'Glad ', 'Ecstatic', 'elated', 'accomplished ', 'Relieved', 'Accomplished ', 'happy', 'proud', 'pleased'], 'ai_responses': []}, 'satisfaction.jpg': {'human_responses': ['Sad', 'Upset ', 'Upset', 'concern', 'sadness ', 'Sad', 'Sad', 'sad', 'curious', 'worried'], 'ai_responses': []}, 'sadness_ai.jpg': {'human_responses': ['Frustration ', 'Stressed ', 'Distressed', 'anguish', 'despair ', 'Sad', 'Disappointed ', 'sad', 'Frustrated', 'anxious'], 'ai_responses': []}, 'shame_ai.jpg': {'human_responses': ['Content ', 'Happy', 'Cordial', 'satisfied', 'smug ', 'Content', 'Blank', 'happy', 'happy'], 'ai_responses': []}, 'satisfaction_ai.jpg': {'human_responses': ['Calm ', 'Giggly ', 'Happy', 'confident', 'happy ', 'Happy', 'Happy', 'nice', 'content', 'confident'], 'ai_responses': []}, 'surprise_ai.jpg': {'human_responses': ['Curious ', 'Intense ', 'Concerned', 'concerned', 'inquisitive ', 'Skeptical', 'Disturbed', 'sad', 'disgusted'], 'ai_responses': []}, 'surprise.jpg': {'human_responses': ['Tired', 'Disappointed ', 'Frustrated', 'frustrated', 'disappointment ', 'Disappointed', 'Disappointed ', 'broken', 'frustrated', 'pensive'], 'ai_responses': []}}
"""


"""
make a python code following the instructions below:

for each image, the emotion corresponding to that image is image_name.split("_")[0]. For each image, make a chart with the human responses and the AI responses represented as data points in a 2D space. The ground truth emotion should be placed in the same space with the color green. Use the color blue for the human responses and red for the AI responses. Calculate the distance between the human responses and the AI responses for each image. Calculate the average distance for all images.

Here is some sample data, assume that ai_responses will have contents for each image:


"""